{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to MrCardist's Documentation","text":"<p>This site is a collection of my knowledge and experiences across various interests and hobbies.</p>"},{"location":"#topics","title":"Topics","text":""},{"location":"#cardistry","title":"Cardistry","text":"<p>The art of card flourishing and manipulation.</p>"},{"location":"#lacrosse","title":"Lacrosse","text":"<p>Information, resources, and notes about lacrosse.</p>"},{"location":"#rocketry","title":"Rocketry","text":"<p>Model rocketry, design, and launch experiences.</p>"},{"location":"#empire-of-the-sun","title":"Empire of the Sun","text":"<p>Information and notes about Empire of the Sun.</p>"},{"location":"#video-games","title":"Video Games","text":"<p>Gaming experiences, reviews, and guides.</p>"},{"location":"#reading","title":"Reading","text":"<p>Book reviews, reading lists, and literary notes.</p> <p>Use the navigation above to explore each topic in detail.</p>"},{"location":"cardistry/","title":"Cardistry","text":"<p>Cardistry is the performance art of card flourishing. Unlike card magic, cardistry is meant to be visually impressive and does not involve illusions or tricks.</p>"},{"location":"cardistry/#getting-started","title":"Getting Started","text":"<p>This section will contain: - Basic moves and techniques - Advanced flourishes - Recommended resources - Practice tips - Video tutorials</p>"},{"location":"cardistry/#techniques","title":"Techniques","text":"<p>Coming soon...</p>"},{"location":"cardistry/#resources","title":"Resources","text":"<p>Coming soon...</p>"},{"location":"empire-of-the-sun/","title":"Empire of the Sun","text":"<p>Information and documentation about Empire of the Sun.</p>"},{"location":"empire-of-the-sun/#overview","title":"Overview","text":"<p>This section will contain: - Background information - Key topics and themes - Notes and observations - References and resources</p>"},{"location":"empire-of-the-sun/#topics","title":"Topics","text":"<p>Coming soon...</p>"},{"location":"empire-of-the-sun/#resources","title":"Resources","text":"<p>Coming soon...</p>"},{"location":"lacrosse/","title":"Lacrosse","text":"<p>Welcome to my lacrosse documentation! This section contains information, resources, and notes about lacrosse.</p>"},{"location":"lacrosse/#overview","title":"Overview","text":"<p>Lacrosse is a fast-paced team sport played with a stick (crosse) and a ball. Players use the stick to catch, carry, and pass the ball with the goal of scoring by shooting the ball into the opponent's goal.</p>"},{"location":"lacrosse/#topics","title":"Topics","text":""},{"location":"lacrosse/#playing-experience","title":"Playing Experience","text":"<ul> <li>Positions and roles</li> <li>Game strategies and tactics</li> <li>Training routines and drills</li> </ul>"},{"location":"lacrosse/#equipment","title":"Equipment","text":"<ul> <li>Sticks (attack, midfield, defense, goalie)</li> <li>Protective gear</li> <li>Ball specifications</li> </ul>"},{"location":"lacrosse/#rules-and-gameplay","title":"Rules and Gameplay","text":"<ul> <li>Field dimensions and layout</li> <li>Game duration and structure</li> <li>Common penalties and violations</li> <li>Face-off techniques</li> </ul>"},{"location":"lacrosse/#skills-development","title":"Skills Development","text":"<ul> <li>Cradling techniques</li> <li>Passing and catching</li> <li>Shooting fundamentals</li> <li>Ground balls and loose ball recovery</li> <li>Defensive positioning</li> </ul>"},{"location":"lacrosse/#teams-and-leagues","title":"Teams and Leagues","text":"<ul> <li>Professional leagues (PLL, NLL)</li> <li>College lacrosse</li> <li>Youth and club programs</li> </ul>"},{"location":"lacrosse/#resources","title":"Resources","text":"<ul> <li>Training videos and tutorials</li> <li>Rule books and guides</li> <li>Equipment recommendations</li> <li>Local programs and clubs</li> </ul> <p>This section is actively being developed. Check back for updates!</p>"},{"location":"llm-proxy/","title":"LLM Proxy","text":"<p>A simple, Go-based reverse proxy for accessing multiple LLM providers through a unified gateway.</p>"},{"location":"llm-proxy/#what-is-llm-proxy","title":"What is LLM Proxy?","text":"<p>LLM Proxy is a lightweight gateway that sits between your applications and LLM providers (OpenAI, Anthropic, Google Gemini), providing:</p> <ul> <li>Unified API - Single endpoint for all LLM providers</li> <li>Provider Routing - Route requests to different providers based on your needs</li> <li>Streaming Support - Native support for streaming responses</li> <li>Cost Tracking - Monitor and control LLM spending</li> <li>Rate Limiting - Prevent abuse and enforce usage limits</li> <li>API Key Management - Secure handling of provider credentials</li> </ul>"},{"location":"llm-proxy/#key-features","title":"Key Features","text":"<ul> <li>\u2705 Multi-provider support: OpenAI, Anthropic, Gemini</li> <li>\u2705 Streaming responses: Efficient handling of streamed completions</li> <li>\u2705 Rate limiting: Token-based and request-based limits</li> <li>\u2705 Cost tracking: Real-time usage monitoring and billing</li> <li>\u2705 CORS support: Browser-based application compatibility</li> <li>\u2705 Health checks: Monitor provider status</li> <li>\u2705 Configurable: Simple YAML configuration</li> </ul>"},{"location":"llm-proxy/#use-cases","title":"Use Cases","text":"<ul> <li>Unified LLM Access: Applications don't need to know which provider to use</li> <li>Cost Control: Track and limit LLM spending per user/team/project</li> <li>Provider Switching: Change LLM providers without updating application code</li> <li>Multi-cloud: Use cloud providers alongside self-hosted models</li> <li>Central Governance: Manage API keys and access policies in one place</li> </ul>"},{"location":"llm-proxy/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Quick Start - Get up and running in minutes</li> <li>API Reference - Available endpoints and request formats</li> <li>Configuration - Setup and configuration options</li> <li>Development - Building and extending the proxy</li> </ul>"},{"location":"llm-proxy/#project-information","title":"Project Information","text":"<ul> <li>Repository: github.com/MrCardist/llm-proxy</li> <li>Language: Go 1.24.5+</li> <li>License: See repository for details</li> </ul>"},{"location":"llm-proxy/api-reference/","title":"API Reference","text":"<p>Complete reference for all LLM Proxy endpoints and request formats.</p>"},{"location":"llm-proxy/api-reference/#general-endpoints","title":"General Endpoints","text":""},{"location":"llm-proxy/api-reference/#health-check","title":"Health Check","text":"<p>Check the status of the proxy and all configured providers.</p> <pre><code>GET /health\n</code></pre> <p>Response: <pre><code>{\n  \"status\": \"healthy\",\n  \"proxy_version\": \"1.0.0\",\n  \"providers\": {\n    \"openai\": \"healthy\",\n    \"anthropic\": \"healthy\",\n    \"gemini\": \"healthy\"\n  }\n}\n</code></pre></p>"},{"location":"llm-proxy/api-reference/#openai-endpoints","title":"OpenAI Endpoints","text":"<p>All OpenAI API endpoints are available through the <code>/openai</code> prefix.</p>"},{"location":"llm-proxy/api-reference/#chat-completions","title":"Chat Completions","text":"<pre><code>POST /openai/v1/chat/completions\n</code></pre> <p>Headers: - <code>Authorization: Bearer YOUR_API_KEY</code> (required) - <code>Content-Type: application/json</code> (required)</p> <p>Request: <pre><code>{\n  \"model\": \"gpt-3.5-turbo\",\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ],\n  \"temperature\": 0.7,\n  \"max_tokens\": 100,\n  \"stream\": false\n}\n</code></pre></p> <p>Streaming: Add <code>\"stream\": true</code> to receive tokens as they're generated.</p> <p>Response: <pre><code>{\n  \"id\": \"chatcmpl-...\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gpt-3.5-turbo\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! How can I help you?\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 15,\n    \"total_tokens\": 25\n  }\n}\n</code></pre></p>"},{"location":"llm-proxy/api-reference/#completions","title":"Completions","text":"<pre><code>POST /openai/v1/completions\n</code></pre> <p>Headers: - <code>Authorization: Bearer YOUR_API_KEY</code> (required) - <code>Content-Type: application/json</code> (required)</p> <p>Request: <pre><code>{\n  \"model\": \"text-davinci-003\",\n  \"prompt\": \"Translate this to French: Hello\",\n  \"max_tokens\": 50,\n  \"stream\": false\n}\n</code></pre></p>"},{"location":"llm-proxy/api-reference/#other-openai-endpoints","title":"Other OpenAI Endpoints","text":"<p>All other OpenAI API endpoints are supported:</p> <pre><code>* /openai/v1/*\n</code></pre>"},{"location":"llm-proxy/api-reference/#anthropic-endpoints","title":"Anthropic Endpoints","text":"<p>All Anthropic API endpoints are available through the <code>/anthropic</code> prefix.</p>"},{"location":"llm-proxy/api-reference/#messages","title":"Messages","text":"<pre><code>POST /anthropic/v1/messages\n</code></pre> <p>Headers: - <code>x-api-key: YOUR_API_KEY</code> (required) - <code>anthropic-version: 2023-06-01</code> (required) - <code>Content-Type: application/json</code> (required)</p> <p>Request: <pre><code>{\n  \"model\": \"claude-3-sonnet-20240229\",\n  \"max_tokens\": 1024,\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello!\"\n    }\n  ],\n  \"stream\": false\n}\n</code></pre></p> <p>Streaming: Add <code>\"stream\": true</code> to receive tokens as they're generated.</p> <p>Response: <pre><code>{\n  \"id\": \"msg_...\",\n  \"type\": \"message\",\n  \"role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Hello! How can I help you today?\"\n    }\n  ],\n  \"model\": \"claude-3-sonnet-20240229\",\n  \"stop_reason\": \"end_turn\",\n  \"stop_sequence\": null,\n  \"usage\": {\n    \"input_tokens\": 10,\n    \"output_tokens\": 15\n  }\n}\n</code></pre></p>"},{"location":"llm-proxy/api-reference/#other-anthropic-endpoints","title":"Other Anthropic Endpoints","text":"<p>All other Anthropic API endpoints are supported:</p> <pre><code>* /anthropic/v1/*\n</code></pre>"},{"location":"llm-proxy/api-reference/#google-gemini-endpoints","title":"Google Gemini Endpoints","text":"<p>All Gemini API endpoints are available through the <code>/gemini</code> prefix.</p>"},{"location":"llm-proxy/api-reference/#generate-content","title":"Generate Content","text":"<pre><code>POST /gemini/v1/models/{model}:generateContent?key=YOUR_API_KEY\n</code></pre> <p>Query Parameters: - <code>key: YOUR_GEMINI_KEY</code> (required)</p> <p>Request: <pre><code>{\n  \"contents\": [\n    {\n      \"parts\": [\n        {\n          \"text\": \"Hello!\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"text\": \"Hello! How can I assist you today?\"\n          }\n        ],\n        \"role\": \"model\"\n      },\n      \"finishReason\": \"STOP\"\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 3,\n    \"candidatesTokenCount\": 10,\n    \"totalTokenCount\": 13\n  }\n}\n</code></pre></p>"},{"location":"llm-proxy/api-reference/#stream-generate-content","title":"Stream Generate Content","text":"<p>For streaming responses:</p> <pre><code>POST /gemini/v1/models/{model}:streamGenerateContent?key=YOUR_API_KEY\n</code></pre>"},{"location":"llm-proxy/api-reference/#other-gemini-endpoints","title":"Other Gemini Endpoints","text":"<p>All other Gemini API endpoints are supported:</p> <pre><code>* /gemini/v1/*\n</code></pre>"},{"location":"llm-proxy/api-reference/#rate-limiting-headers","title":"Rate Limiting Headers","text":"<p>When rate limiting is enabled, responses include:</p> <pre><code>X-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 950\nX-RateLimit-Reset: 1234567890\n</code></pre> <p>If you exceed the rate limit, you'll receive:</p> <pre><code>HTTP/1.1 429 Too Many Requests\nRetry-After: 60\nX-RateLimit-Remaining: 0\n</code></pre>"},{"location":"llm-proxy/api-reference/#error-responses","title":"Error Responses","text":""},{"location":"llm-proxy/api-reference/#invalid-api-key","title":"Invalid API Key","text":"<pre><code>{\n  \"error\": \"Invalid API key: API key not found\"\n}\n</code></pre> <p>Status: <code>401 Unauthorized</code></p>"},{"location":"llm-proxy/api-reference/#rate-limit-exceeded","title":"Rate Limit Exceeded","text":"<pre><code>{\n  \"error\": \"Rate limit exceeded\"\n}\n</code></pre> <p>Status: <code>429 Too Many Requests</code></p>"},{"location":"llm-proxy/api-reference/#provider-error","title":"Provider Error","text":"<p>Errors from the LLM provider are passed through with the original status code.</p>"},{"location":"llm-proxy/api-reference/#cors-support","title":"CORS Support","text":"<p>The proxy includes CORS headers for browser-based requests:</p> <pre><code>Access-Control-Allow-Origin: *\nAccess-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS\nAccess-Control-Allow-Headers: Content-Type, Authorization, x-api-key\n</code></pre>"},{"location":"llm-proxy/configuration/","title":"Configuration","text":"<p>Configure LLM Proxy to suit your deployment environment and requirements.</p>"},{"location":"llm-proxy/configuration/#configuration-files","title":"Configuration Files","text":"<p>The proxy uses YAML configuration files located in the <code>configs/</code> directory:</p> <ul> <li><code>base.yml</code> - Default configuration</li> <li><code>dev.yml</code> - Development environment overrides</li> <li><code>staging.yml</code> - Staging environment overrides</li> <li><code>production.yml</code> - Production environment overrides</li> </ul> <p>The appropriate config file is selected based on the <code>ENVIRONMENT</code> variable (default: <code>dev</code>).</p>"},{"location":"llm-proxy/configuration/#basic-configuration","title":"Basic Configuration","text":""},{"location":"llm-proxy/configuration/#server-settings","title":"Server Settings","text":"<pre><code>server:\n  port: 9002  # Port to listen on\n</code></pre>"},{"location":"llm-proxy/configuration/#environment-variables","title":"Environment Variables","text":"<p>Override configuration with environment variables:</p> <pre><code># Port\nexport PORT=9002\n\n# Providers\nexport OPENAI_API_KEY=\"sk-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\nexport GEMINI_API_KEY=\"...\"\n\n# Environment selection\nexport ENVIRONMENT=production\n</code></pre>"},{"location":"llm-proxy/configuration/#features","title":"Features","text":""},{"location":"llm-proxy/configuration/#rate-limiting","title":"Rate Limiting","text":"<p>Enable token-based and request-based rate limiting:</p> <pre><code>features:\n  rate_limiting:\n    enabled: true\n    backend: \"memory\"  # Only 'memory' is available for single instances\n    estimation:\n      max_sample_bytes: 20000\n      bytes_per_token: 4\n      chars_per_token: 4\n      provider_chars_per_token:\n        openai: 5        # ~185-190 tokens per 1k chars\n        anthropic: 3     # ~290-315 tokens per 1k chars\n        gemini: 4        # Estimated\n    limits:\n      requests_per_minute: 100\n      tokens_per_minute: 100000\n</code></pre> <p>How it works: - Limits are per API key - Token counts are estimated from request size - Actual token usage is reconciled from provider responses - First request in a window is always allowed even if it would exceed limits</p>"},{"location":"llm-proxy/configuration/#api-key-management","title":"API Key Management","text":"<p>Secure management of provider API keys with proxy-specific keys:</p> <pre><code>features:\n  api_key_management:\n    enabled: true\n    table_name: \"llm-proxy-api-keys\"\n    region: \"us-west-2\"\n</code></pre> <p>Benefits: - Generate proxy-specific keys prefixed with <code>iw:</code> - Store actual provider keys securely in DynamoDB - Enable/disable keys without deleting them - Set cost limits per key (future implementation) - Track usage and metadata</p> <p>See the API Key Management guide in the repository for detailed setup.</p>"},{"location":"llm-proxy/configuration/#cost-tracking","title":"Cost Tracking","text":"<p>Monitor and track LLM usage and costs:</p> <pre><code>features:\n  cost_tracking:\n    enabled: true\n    backend: \"dynamodb\"  # Options: dynamodb, datadog, file\n</code></pre>"},{"location":"llm-proxy/configuration/#available-backends","title":"Available Backends","text":"<ul> <li>file: Store costs locally (development only)</li> <li>dynamodb: Store costs in AWS DynamoDB</li> <li>datadog: Send costs to Datadog for monitoring</li> </ul>"},{"location":"llm-proxy/configuration/#provider-configuration","title":"Provider Configuration","text":""},{"location":"llm-proxy/configuration/#openai","title":"OpenAI","text":"<pre><code>providers:\n  openai:\n    enabled: true\n    api_endpoint: \"https://api.openai.com\"\n    timeout: 30  # seconds\n</code></pre>"},{"location":"llm-proxy/configuration/#anthropic","title":"Anthropic","text":"<pre><code>providers:\n  anthropic:\n    enabled: true\n    api_endpoint: \"https://api.anthropic.com\"\n    timeout: 30  # seconds\n</code></pre>"},{"location":"llm-proxy/configuration/#google-gemini","title":"Google Gemini","text":"<pre><code>providers:\n  gemini:\n    enabled: true\n    api_endpoint: \"https://generativelanguage.googleapis.com\"\n    timeout: 30  # seconds\n</code></pre>"},{"location":"llm-proxy/configuration/#logging","title":"Logging","text":"<p>Configure request/response logging:</p> <pre><code>logging:\n  level: \"info\"  # Options: debug, info, warn, error\n  format: \"json\"  # Options: json, text\n</code></pre>"},{"location":"llm-proxy/configuration/#development-configuration-example","title":"Development Configuration Example","text":"<p>Full <code>dev.yml</code> example:</p> <pre><code>server:\n  port: 9002\n\nproviders:\n  openai:\n    enabled: true\n  anthropic:\n    enabled: true\n  gemini:\n    enabled: true\n\nfeatures:\n  rate_limiting:\n    enabled: false  # Disabled in dev\n  api_key_management:\n    enabled: false  # Disabled in dev\n\nlogging:\n  level: \"debug\"\n  format: \"text\"\n</code></pre>"},{"location":"llm-proxy/configuration/#production-configuration-example","title":"Production Configuration Example","text":"<p>Secure <code>production.yml</code> example:</p> <pre><code>server:\n  port: 9002\n\nproviders:\n  openai:\n    enabled: true\n    timeout: 30\n  anthropic:\n    enabled: true\n    timeout: 30\n  gemini:\n    enabled: true\n    timeout: 30\n\nfeatures:\n  rate_limiting:\n    enabled: true\n    backend: \"memory\"\n    limits:\n      requests_per_minute: 100\n      tokens_per_minute: 500000\n  api_key_management:\n    enabled: true\n    table_name: \"llm-proxy-api-keys\"\n    region: \"us-west-2\"\n  cost_tracking:\n    enabled: true\n    backend: \"dynamodb\"\n\nlogging:\n  level: \"info\"\n  format: \"json\"\n</code></pre>"},{"location":"llm-proxy/configuration/#deployment-with-docker","title":"Deployment with Docker","text":"<p>The proxy includes Docker support for easy deployment:</p> <pre><code># Build the image\ndocker build -f Dockerfile -t llm-proxy:latest .\n\n# Run the container\ndocker run -p 9002:9002 \\\n  -e ENVIRONMENT=production \\\n  -e OPENAI_API_KEY=\"sk-...\" \\\n  -e ANTHROPIC_API_KEY=\"sk-ant-...\" \\\n  -e GEMINI_API_KEY=\"...\" \\\n  llm-proxy:latest\n</code></pre>"},{"location":"llm-proxy/configuration/#docker-compose","title":"Docker Compose","text":"<p>For development with multiple services:</p> <pre><code>docker-compose up\n</code></pre> <p>See the repository for complete <code>docker-compose.yml</code> examples.</p>"},{"location":"llm-proxy/configuration/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Use HTTPS - Always use HTTPS in production</li> <li>Secure Credentials - Store API keys in environment variables or secrets management systems</li> <li>API Key Rotation - Regularly rotate both provider and proxy API keys</li> <li>Rate Limiting - Enable rate limiting to prevent abuse</li> <li>Access Control - Use AWS IAM to restrict DynamoDB table access</li> <li>Monitoring - Monitor logs and set up alerts for failures</li> </ol>"},{"location":"llm-proxy/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"llm-proxy/configuration/#health-check-failing","title":"Health Check Failing","text":"<pre><code>curl http://localhost:9002/health\n</code></pre> <p>If a provider shows unhealthy, check that the API key is set correctly and has valid credentials.</p>"},{"location":"llm-proxy/configuration/#rate-limiting-too-strict","title":"Rate Limiting Too Strict","text":"<p>Adjust token estimation values in <code>chars_per_token</code> to match your usage patterns, or increase rate limit values.</p>"},{"location":"llm-proxy/configuration/#high-latency","title":"High Latency","text":"<p>The proxy overhead should be &lt;10ms. If you see higher latency: - Check network connectivity to providers - Verify provider API isn't rate limiting you - Check server CPU/memory availability</p>"},{"location":"llm-proxy/development/","title":"Development","text":"<p>Information for developers who want to build, test, and extend LLM Proxy.</p>"},{"location":"llm-proxy/development/#development-setup","title":"Development Setup","text":""},{"location":"llm-proxy/development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go 1.24.5 or later</li> <li>Make (included on macOS/Linux; use MinGW on Windows)</li> <li>API keys for testing (optional, but recommended)</li> </ul>"},{"location":"llm-proxy/development/#getting-started","title":"Getting Started","text":"<pre><code># Clone the repository\ngit clone https://github.com/MrCardist/llm-proxy.git\ncd llm-proxy\n\n# Install dependencies\nmake install\n\n# Build the binary\nmake build\n\n# Run in development mode\nmake dev\n</code></pre>"},{"location":"llm-proxy/development/#available-make-commands","title":"Available Make Commands","text":""},{"location":"llm-proxy/development/#code-quality","title":"Code Quality","text":"<pre><code>make check      # Run all code quality checks\nmake fmt        # Format Go code\nmake vet        # Run go vet analysis\nmake lint       # Run golint\n</code></pre>"},{"location":"llm-proxy/development/#building","title":"Building","text":"<pre><code>make build      # Build the binary (outputs to ./bin/llm-proxy)\nmake clean      # Clean build artifacts\nmake install    # Install Go dependencies\nmake help       # Show all available commands\n</code></pre>"},{"location":"llm-proxy/development/#running","title":"Running","text":"<pre><code>make run        # Run the built binary\nmake dev        # Run in development mode with auto-reload\nmake stop       # Stop running proxy\n</code></pre>"},{"location":"llm-proxy/development/#testing","title":"Testing","text":"<pre><code>make test           # Run unit tests only\nmake test-all       # Run all tests (unit + integration)\nmake test-openai    # Run OpenAI integration tests\nmake test-anthropic # Run Anthropic integration tests\nmake test-gemini    # Run Gemini integration tests\nmake test-health    # Run health check tests\nmake env-check      # Verify environment variables\n</code></pre>"},{"location":"llm-proxy/development/#project-structure","title":"Project Structure","text":"<pre><code>llm-proxy/\n\u251c\u2500\u2500 cmd/\n\u2502   \u251c\u2500\u2500 llm-proxy/\n\u2502   \u2502   \u2514\u2500\u2500 main.go           # Server entry point\n\u2502   \u2514\u2500\u2500 llm-proxy-keys/\n\u2502       \u2514\u2500\u2500 main.go           # Key management CLI tool\n\u251c\u2500\u2500 internal/\n\u2502   \u251c\u2500\u2500 providers/\n\u2502   \u2502   \u251c\u2500\u2500 provider.go       # Provider interface\n\u2502   \u2502   \u251c\u2500\u2500 openai.go         # OpenAI implementation\n\u2502   \u2502   \u251c\u2500\u2500 anthropic.go      # Anthropic implementation\n\u2502   \u2502   \u2514\u2500\u2500 gemini.go         # Gemini implementation\n\u2502   \u251c\u2500\u2500 middleware/\n\u2502   \u2502   \u251c\u2500\u2500 logging.go        # Request logging\n\u2502   \u2502   \u251c\u2500\u2500 cors.go           # CORS headers\n\u2502   \u2502   \u2514\u2500\u2500 streaming.go      # Streaming detection\n\u2502   \u251c\u2500\u2500 ratelimit/\n\u2502   \u2502   \u2514\u2500\u2500 memory.go         # Rate limiting logic\n\u2502   \u251c\u2500\u2500 cost/\n\u2502   \u2502   \u2514\u2500\u2500 tracker.go        # Cost tracking\n\u2502   \u2514\u2500\u2500 config/\n\u2502       \u2514\u2500\u2500 config.go         # Configuration loading\n\u251c\u2500\u2500 configs/\n\u2502   \u251c\u2500\u2500 base.yml              # Default configuration\n\u2502   \u251c\u2500\u2500 dev.yml               # Development overrides\n\u2502   \u251c\u2500\u2500 staging.yml           # Staging overrides\n\u2502   \u2514\u2500\u2500 production.yml        # Production settings\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 openai_test.go        # OpenAI tests\n\u2502   \u251c\u2500\u2500 anthropic_test.go     # Anthropic tests\n\u2502   \u251c\u2500\u2500 gemini_test.go        # Gemini tests\n\u2502   \u251c\u2500\u2500 common_test.go        # Shared tests\n\u2502   \u2514\u2500\u2500 test_helpers.go       # Test utilities\n\u2514\u2500\u2500 Makefile                  # Build and development commands\n</code></pre>"},{"location":"llm-proxy/development/#setting-up-for-testing","title":"Setting Up for Testing","text":""},{"location":"llm-proxy/development/#environment-variables","title":"Environment Variables","text":"<p>Export API keys for integration testing:</p> <pre><code>export OPENAI_API_KEY=\"your-openai-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\nexport GEMINI_API_KEY=\"your-gemini-key\"\n</code></pre>"},{"location":"llm-proxy/development/#run-tests","title":"Run Tests","text":"<pre><code># All tests (requires all API keys)\nmake test-all\n\n# Specific provider\nmake test-openai\n\n# Check if environment is set up correctly\nmake env-check\n</code></pre>"},{"location":"llm-proxy/development/#test-structure","title":"Test Structure","text":"<p>Each provider has integration tests in <code>*_test.go</code>: - Streaming tests - Verify streaming responses work correctly - Non-streaming tests - Verify regular completions work - Error handling tests - Verify error responses are handled - Metadata parsing - Verify token counts and response metadata</p>"},{"location":"llm-proxy/development/#architecture","title":"Architecture","text":""},{"location":"llm-proxy/development/#request-flow","title":"Request Flow","text":"<ol> <li>Client Request \u2192 HTTP server receives request</li> <li>Middleware Stack</li> <li>CORS headers added</li> <li>Request logging</li> <li>Streaming detection</li> <li>Rate limiting enforcement</li> <li>Token estimation</li> <li>Provider Router \u2192 Routes to appropriate provider (OpenAI, Anthropic, Gemini)</li> <li>Provider Handler \u2192 Translates request to provider API format</li> <li>External API Call \u2192 Calls the LLM provider</li> <li>Response Processing</li> <li>Parse response</li> <li>Extract metadata (tokens, costs)</li> <li>Handle streaming if needed</li> <li>Cost Tracking \u2192 Log usage and costs</li> <li>Response Returned \u2192 Client receives response</li> </ol>"},{"location":"llm-proxy/development/#provider-interface","title":"Provider Interface","text":"<p>Each provider implements:</p> <pre><code>type Provider interface {\n    // Handle processes a request through the provider\n    Handle(w http.ResponseWriter, r *http.Request)\n\n    // IsStreamingRequest detects if request asks for streaming\n    IsStreamingRequest(body []byte) bool\n\n    // Health checks provider availability\n    Health() (bool, string)\n}\n</code></pre>"},{"location":"llm-proxy/development/#adding-a-new-provider","title":"Adding a New Provider","text":"<p>To add support for a new LLM provider:</p>"},{"location":"llm-proxy/development/#1-create-provider-implementation","title":"1. Create Provider Implementation","text":"<p>Create <code>internal/providers/newprovider.go</code>:</p> <pre><code>package providers\n\nimport (\n    \"net/http\"\n)\n\ntype NewProviderImpl struct {\n    config *ProviderConfig\n}\n\nfunc (p *NewProviderImpl) Handle(w http.ResponseWriter, r *http.Request) {\n    // Translate request\n    // Call provider API\n    // Handle streaming\n    // Write response\n}\n\nfunc (p *NewProviderImpl) IsStreamingRequest(body []byte) bool {\n    // Check if request asks for streaming\n    return false\n}\n\nfunc (p *NewProviderImpl) Health() (bool, string) {\n    // Check provider health\n    return true, \"healthy\"\n}\n</code></pre>"},{"location":"llm-proxy/development/#2-create-tests","title":"2. Create Tests","text":"<p>Create <code>tests/newprovider_test.go</code> with streaming and non-streaming tests.</p>"},{"location":"llm-proxy/development/#3-register-provider","title":"3. Register Provider","text":"<p>Add to <code>cmd/llm-proxy/main.go</code>:</p> <pre><code>router.HandleFunc(\"/newprovider/v1/{path:.*}\", providers.NewProvider.Handle)\n</code></pre>"},{"location":"llm-proxy/development/#4-add-configuration","title":"4. Add Configuration","text":"<p>Add to <code>configs/base.yml</code>:</p> <pre><code>providers:\n  newprovider:\n    enabled: false\n    api_endpoint: \"https://api.newprovider.com\"\n    timeout: 30\n</code></pre>"},{"location":"llm-proxy/development/#debugging","title":"Debugging","text":""},{"location":"llm-proxy/development/#enable-debug-logging","title":"Enable Debug Logging","text":"<p>Set <code>logging.level: \"debug\"</code> in your config file or:</p> <pre><code>LOG_LEVEL=debug make dev\n</code></pre>"},{"location":"llm-proxy/development/#inspect-requests","title":"Inspect Requests","text":"<p>The proxy logs all requests with: - Request path and method - Headers (sensitive headers are masked) - Request size - Streaming detection - Provider routing</p>"},{"location":"llm-proxy/development/#common-issues","title":"Common Issues","text":"<p>Provider returns 401: Check that the API key is correct and has valid credentials</p> <p>Slow responses: Check network connectivity to provider and provider status page</p> <p>Rate limiting too strict: Adjust <code>chars_per_token</code> values based on your content type</p>"},{"location":"llm-proxy/development/#benchmarking","title":"Benchmarking","text":"<p>The repository includes token estimation benchmarks:</p> <pre><code>python scripts/token_estimation.py\n</code></pre> <p>This generates a table of estimated tokens per 1k characters for each provider, helping tune rate limiting settings.</p>"},{"location":"llm-proxy/development/#release-process","title":"Release Process","text":"<ol> <li>Update version in <code>cmd/llm-proxy/main.go</code></li> <li>Commit changes with version bump</li> <li>Tag commit: <code>git tag v1.2.3</code></li> <li>Push: <code>git push origin main --tags</code></li> <li>CircleCI automatically builds and publishes release</li> </ol>"},{"location":"llm-proxy/development/#deployment","title":"Deployment","text":""},{"location":"llm-proxy/development/#docker","title":"Docker","text":"<pre><code># Development\ndocker-compose up\n\n# Production\ndocker build -f Dockerfile.prod -t llm-proxy:v1.0.0 .\ndocker push your-registry/llm-proxy:v1.0.0\n</code></pre>"},{"location":"llm-proxy/development/#kubernetes","title":"Kubernetes","text":"<p>The proxy is stateless and easily deployable to Kubernetes:</p> <pre><code>kubectl apply -f k8s/deployment.yaml\nkubectl apply -f k8s/service.yaml\n</code></pre> <p>See the repository for example K8s manifests.</p>"},{"location":"llm-proxy/development/#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please:</p> <ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Write tests for your changes</li> <li>Ensure <code>make check</code> passes</li> <li>Submit a pull request</li> </ol>"},{"location":"llm-proxy/development/#performance-targets","title":"Performance Targets","text":"<ul> <li>Request Processing: &lt;10ms overhead per request</li> <li>Token Estimation: &lt;5% error margin</li> <li>Cost Tracking: &lt;1% error margin</li> <li>Throughput: Support thousands of concurrent requests</li> </ul>"},{"location":"llm-proxy/quick-start/","title":"Quick Start","text":"<p>Get LLM Proxy up and running in a few minutes.</p>"},{"location":"llm-proxy/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go 1.24.5 or later</li> <li>API keys from your chosen providers (OpenAI, Anthropic, and/or Gemini)</li> </ul>"},{"location":"llm-proxy/quick-start/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"llm-proxy/quick-start/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/MrCardist/llm-proxy.git\ncd llm-proxy\n</code></pre>"},{"location":"llm-proxy/quick-start/#2-install-dependencies","title":"2. Install Dependencies","text":"<pre><code># Using the Makefile\nmake install\n\n# Or manually\ngo mod download\n</code></pre>"},{"location":"llm-proxy/quick-start/#3-set-up-api-keys","title":"3. Set Up API Keys","text":"<p>Export your LLM provider API keys as environment variables:</p> <pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\nexport GEMINI_API_KEY=\"...\"\n</code></pre>"},{"location":"llm-proxy/quick-start/#4-start-the-proxy","title":"4. Start the Proxy","text":"<pre><code># Production build\nmake install build\nmake run\n\n# Or development mode with auto-reload\nmake dev\n</code></pre> <p>The proxy starts on <code>http://localhost:9002</code></p>"},{"location":"llm-proxy/quick-start/#making-your-first-request","title":"Making Your First Request","text":""},{"location":"llm-proxy/quick-start/#health-check","title":"Health Check","text":"<p>Verify the proxy is running and check provider status:</p> <pre><code>curl http://localhost:9002/health\n</code></pre>"},{"location":"llm-proxy/quick-start/#openai-chat-completion","title":"OpenAI Chat Completion","text":"<pre><code>curl -X POST http://localhost:9002/openai/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer YOUR_OPENAI_KEY\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n    \"max_tokens\": 50\n  }'\n</code></pre>"},{"location":"llm-proxy/quick-start/#anthropic-messages","title":"Anthropic Messages","text":"<pre><code>curl -X POST http://localhost:9002/anthropic/v1/messages \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: YOUR_ANTHROPIC_KEY\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -d '{\n    \"model\": \"claude-3-sonnet-20240229\",\n    \"max_tokens\": 100,\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n</code></pre>"},{"location":"llm-proxy/quick-start/#google-gemini","title":"Google Gemini","text":"<pre><code>curl -X POST http://localhost:9002/gemini/v1/models/gemini-pro:generateContent?key=YOUR_GEMINI_KEY \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\"parts\": [{\"text\": \"Hello!\"}]}]\n  }'\n</code></pre>"},{"location":"llm-proxy/quick-start/#streaming-responses","title":"Streaming Responses","text":"<p>Add <code>\"stream\": true</code> to your request to get streamed responses:</p> <pre><code>curl -X POST http://localhost:9002/openai/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer YOUR_OPENAI_KEY\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"llm-proxy/quick-start/#running-tests","title":"Running Tests","text":"<p>Test that everything is configured correctly:</p> <pre><code># All tests\nmake test-all\n\n# Provider-specific tests\nmake test-openai\nmake test-anthropic\nmake test-gemini\n\n# Health check tests only\nmake test-health\n</code></pre>"},{"location":"llm-proxy/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Read the Configuration guide to customize the proxy</li> <li>Check the API Reference for all available endpoints</li> <li>See Development for building custom providers or features</li> </ul>"},{"location":"reading/","title":"Reading","text":"<p>Book reviews, reading lists, and literary exploration.</p>"},{"location":"reading/#overview","title":"Overview","text":"<p>This section will contain: - Book reviews and ratings - Reading lists and recommendations - Literary analysis - Author notes - Reading goals and progress</p>"},{"location":"reading/#current-reading","title":"Current Reading","text":"<p>Coming soon...</p>"},{"location":"reading/#reviews","title":"Reviews","text":"<p>Coming soon...</p>"},{"location":"reading/#reading-lists","title":"Reading Lists","text":"<p>Coming soon...</p>"},{"location":"reading/#favorites","title":"Favorites","text":"<p>Coming soon...</p>"},{"location":"rocketry/","title":"Rocketry","text":"<p>Model rocketry combines science, engineering, and the thrill of launch.</p>"},{"location":"rocketry/#overview","title":"Overview","text":"<p>This section will contain: - Rocket designs and builds - Launch logs and experiences - Safety guidelines - Materials and suppliers - Technical specifications</p>"},{"location":"rocketry/#builds","title":"Builds","text":"<p>Coming soon...</p>"},{"location":"rocketry/#launch-logs","title":"Launch Logs","text":"<p>Coming soon...</p>"},{"location":"rocketry/#resources","title":"Resources","text":"<p>Coming soon...</p>"},{"location":"video-games/","title":"Video Games","text":"<p>Gaming experiences, reviews, strategies, and guides.</p>"},{"location":"video-games/#overview","title":"Overview","text":"<p>This section will contain: - Game reviews and impressions - Strategy guides and tips - Gaming achievements - Favorite games - Gaming setups</p>"},{"location":"video-games/#reviews","title":"Reviews","text":"<p>Coming soon...</p>"},{"location":"video-games/#guides","title":"Guides","text":"<p>Coming soon...</p>"},{"location":"video-games/#favorites","title":"Favorites","text":"<p>Coming soon...</p>"}]}